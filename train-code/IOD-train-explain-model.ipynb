{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d474c7-8a94-4491-b616-d91c5465a9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6647219, 45)\n",
      "['nprocs', 'POSIX_OPENS', 'LUSTRE_STRIPE_SIZE', 'LUSTRE_STRIPE_WIDTH', 'POSIX_FILENOS', 'POSIX_MEM_ALIGNMENT', 'POSIX_FILE_ALIGNMENT', 'POSIX_READS', 'POSIX_WRITES', 'POSIX_SEEKS', 'POSIX_STATS', 'POSIX_BYTES_READ', 'POSIX_BYTES_WRITTEN', 'POSIX_CONSEC_READS', 'POSIX_CONSEC_WRITES', 'POSIX_SEQ_READS', 'POSIX_SEQ_WRITES', 'POSIX_RW_SWITCHES', 'POSIX_MEM_NOT_ALIGNED', 'POSIX_FILE_NOT_ALIGNED', 'POSIX_SIZE_READ_0_100', 'POSIX_SIZE_READ_100_1K', 'POSIX_SIZE_READ_1K_10K', 'POSIX_SIZE_READ_100K_1M', 'POSIX_SIZE_WRITE_0_100', 'POSIX_SIZE_WRITE_100_1K', 'POSIX_SIZE_WRITE_1K_10K', 'POSIX_SIZE_WRITE_10K_100K', 'POSIX_SIZE_WRITE_100K_1M', 'POSIX_STRIDE1_STRIDE', 'POSIX_STRIDE2_STRIDE', 'POSIX_STRIDE3_STRIDE', 'POSIX_STRIDE4_STRIDE', 'POSIX_STRIDE1_COUNT', 'POSIX_STRIDE2_COUNT', 'POSIX_STRIDE3_COUNT', 'POSIX_STRIDE4_COUNT', 'POSIX_ACCESS1_ACCESS', 'POSIX_ACCESS2_ACCESS', 'POSIX_ACCESS3_ACCESS', 'POSIX_ACCESS4_ACCESS', 'POSIX_ACCESS1_COUNT', 'POSIX_ACCESS2_COUNT', 'POSIX_ACCESS3_COUNT', 'POSIX_ACCESS4_COUNT']\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#%run -i IOD-model.ipynb\n",
    "%run -i common.ipynb\n",
    "\n",
    "from lime import lime_tabular\n",
    "from numpy import loadtxt\n",
    "import dill\n",
    "import joblib\n",
    "import shap\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "#data_to_train=\"/project/projectdirs/m2621/dbin/Cori_archive_2019_un_taz_parsered_tagged.csv\"\n",
    "\n",
    "#explain_model_save_file=\"/global/cfs/cdirs/m2621/dbin/io-explain-model-lime.joblib\"\n",
    "#explain_model_save_file_shap=\"/global/cfs/cdirs/m2621/dbin/io-explain-model-shap.joblib\"\n",
    "#predict_model_save_file=\"/global/cfs/cdirs/m2621/dbin/io-ai-model.joblib\"\n",
    "\n",
    "#v1\n",
    "#data_to_train=\"/global/cfs/projectdirs/m2621/dbin/Cori_archive_19_20_21_22_un_taz_parsered_tagged_for_training.csv\"\n",
    "data_to_train=\"/global/cfs/projectdirs/m2621/dbin/Cori_archive_19_20_21_22_un_taz_parsered_tagged_for_training-v1.csv\"\n",
    "\n",
    "#predict_model_save_file_xgboost=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-xgboost-20221019-162202.joblib\"\n",
    "\n",
    "\n",
    "\n",
    "# This is the file for Docker version now\n",
    "##predict_model_save_file_xgboost=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-xgboost-sparse-20221107-151140.joblib\"\n",
    "## v0 predict_model_save_file_xgboost=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-xgb-sparse-20221214-111321.joblib\"\n",
    "\n",
    "predict_model_save_file_xgboost=\"/global/homes/d/dbin/IODiagnoser/web/model/io-ai-model-xgb-sparse-20230118-213200-v1.joblib\"\n",
    "explain_model_save_file_shap_xgboost=\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-shap-xgboost-v1.joblib\"\n",
    "explain_model_save_file_lime_xgboost=\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-lime-xgboost-v1.joblib\"\n",
    "\n",
    "\n",
    "# This is the file for Docker version now\n",
    "#predict_model_save_file_tabnet=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-tabnet-cpu-20221102-132100.joblib\"\n",
    "#predict_model_save_file_tabnet=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-tabnet-20221216-231906.joblib\"\n",
    "#explain_model_save_file_shap_tabnet =\"/global/cfs/cdirs/m2621/dbin/io-explain-model-shap-tabnet.joblib\"\n",
    "#explain_model_save_file_lime_tabnet =\"/global/cfs/cdirs/m2621/dbin/io-explain-model-lime-tabnet.joblib\"\n",
    "\n",
    "predict_model_save_file_tabnet=\"/global/homes/d/dbin/IODiagnoser/web/model/io-ai-model-tabnet-20230119-061549-v1.joblib\"\n",
    "explain_model_save_file_lime_tabnet =\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-lime-tabnet-v1.joblib\"\n",
    "\n",
    "\n",
    "# This is the file for Docker version now\n",
    "#predict_model_save_file_mlp=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-mlp-sparse-20221108-110917.joblib\"\n",
    "#v0\n",
    "#predict_model_save_file_mlp=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-mlp-sparse-20221211-094606.joblib\"\n",
    "#v1\n",
    "predict_model_save_file_mlp         =\"/global/homes/d/dbin/IODiagnoser/web/model/io-ai-model-mlp-sparse-20230118-212203-v1.joblib\"\n",
    "explain_model_save_file_shap_mlp    =\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-shap-mlp-v1.joblib\"\n",
    "explain_model_save_file_lime_mlp    =\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-lime-mlp-v1.joblib\"\n",
    "\n",
    "\n",
    "#v0\n",
    "#predict_model_save_file_lightgbm=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-lightGBM-sparse-20230102-173739.joblib\"\n",
    "#explain_model_save_file_shap_lightgbm=\"/global/cfs/cdirs/m2621/dbin/io-explain-model-shap-lightgbm.joblib\"\n",
    "#explain_model_save_file_lime_lightgbm=\"/global/cfs/cdirs/m2621/dbin/io-explain-model-lime-lightgbm.joblib\"\n",
    "\n",
    "predict_model_save_file_lightgbm=\"/global/homes/d/dbin/IODiagnoser/web/model/io-ai-model-lightgbm-sparse-20230119-120849-v1.joblib\"\n",
    "explain_model_save_file_shap_lightgbm=\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-shap-lightgbm-v1.joblib\"\n",
    "explain_model_save_file_lime_lightgbm=\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-lime-lightgbm-v1.joblib\"\n",
    "\n",
    "\n",
    "#predict_model_save_file_catboost=\"/global/cfs/cdirs/m2621/dbin/io-ai-model-catboost-sparse-20230105-214802.joblib\"\n",
    "predict_model_save_file_catboost     =\"/global/homes/d/dbin/IODiagnoser/web/model/io-ai-model-catboost-sparse-20230119-120849-v1.joblib\"\n",
    "explain_model_save_file_shap_catboost=\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-shap-catboost-v1.joblib\"\n",
    "explain_model_save_file_lime_catboost=\"/global/homes/d/dbin/IODiagnoser/web/model/io-explain-model-lime-catboost-v1.joblib\"\n",
    "\n",
    "\n",
    "def load_train_data_to_explain_model(file_csv_to_train, method=\"LIME\"):\n",
    "    if method == \"LIME\":\n",
    "        ## Get the headers from the explain dataset\n",
    "        f = open(file_csv_to_train)\n",
    "        header = f.readline()\n",
    "        dataset_headers = header.split(',')\n",
    "        dataset_headers.pop()\n",
    "        f.close()\n",
    "        ## Get the data used to train the model\n",
    "        train_dataset = loadtxt(file_csv_to_train, delimiter=',', skiprows=1) ## skiprows=1  to skip header\n",
    "        n_dims = train_dataset.shape[1]\n",
    "        train_X = train_dataset[:,0:n_dims-1]\n",
    "        ##train_Y = train_dataset[:,n_dims-1]\n",
    "        return train_X, dataset_headers\n",
    "    elif method == \"SHAP\":\n",
    "        df=pd.read_csv(file_csv_to_train)\n",
    "        #train_Y=df[\"tag\"].to_numpy()\n",
    "        df.drop(['tag'], axis=1, inplace=True)\n",
    "        dataset_headers=list(df.columns)\n",
    "        return df, dataset_headers\n",
    "    else:\n",
    "        print(\"Not supported explain method now !\\n\")\n",
    "        exit()\n",
    "        \n",
    "def create_save_explain_model_LIME(file_to_save, is_train_x_dense=False):\n",
    "    ## Build the LimeTabularExplainer\n",
    "    train_X, dataset_headers=load_train_data_to_explain_model(data_to_train, method=\"LIME\")\n",
    "    print(train_X.shape)\n",
    "    print(dataset_headers)\n",
    "    if is_train_x_dense:\n",
    "        train_X_to_go = train_X\n",
    "    else:\n",
    "        train_X_to_go = sp.sparse.csr_matrix(train_X)\n",
    "\n",
    "    explainer = lime_tabular.LimeTabularExplainer(train_X_to_go, mode=\"regression\", feature_names= dataset_headers)\n",
    "    with open(file_to_save, 'wb') as f:\n",
    "        dill.dump(explainer, f)\n",
    "    #with open('data', 'rb') as f:\n",
    "    #dill.load(f)\n",
    "\n",
    "    \n",
    "## Good example of shap\n",
    "## https://github.com/slundberg/shap/issues/201\n",
    "## https://towardsdatascience.com/explainable-ai-xai-with-shap-regression-problem-b2d63fdca670\n",
    "## https://www.yourdatateacher.com/2021/05/17/how-to-explain-neural-networks-using-shap/\n",
    "def create_save_explain_model_SHAP(file_to_save, predict_model_file_p, is_train_x_dense=False):\n",
    "    train_X, dataset_headers=load_train_data_to_explain_model(data_to_train, method=\"LIME\")\n",
    "    print(train_X.shape)\n",
    "    print(dataset_headers)\n",
    "    ## Since we already have CPU version of Tabnet, we only need load it as normal joblib file\n",
    "    predict_model = load_predict_model(predict_model_file_p, isTabnet = False); \n",
    "    if is_train_x_dense == True:\n",
    "        explainer = shap.Explainer(predict_model.predict) \n",
    "    else:\n",
    "        # https://github.com/slundberg/shap/blob/master/tests/explainers/test_kernel.py#L76\n",
    "        # follow example of test_kernel_shap_with_a1a_sparse_zero_background():\n",
    "        _, cols = train_X.shape\n",
    "        shape = 1, cols\n",
    "        print(\"shape =\", shape)\n",
    "        background = sp.sparse.csr_matrix(shape, dtype=train_X.dtype)\n",
    "        #print(background.toarray())\n",
    "        explainer = shap.KernelExplainer(predict_model.predict, background) \n",
    "    \n",
    "    joblib.dump(explainer, filename=file_to_save)\n",
    "    \n",
    "    #train_X_summary=shap.sample(train_X, 3000)\n",
    "    #masker = shap.maskers.Independent(data = train_X_summary)\n",
    "    #shap_values2 = explainer(train_X.head(10))\n",
    "    #shap.plots.waterfall(shap_values2[0])\n",
    "    ##shap.plots.waterfall(shap_values2[0])\n",
    "    #train_X_summary = shap.kmeans(train_X, 300000)\n",
    "    #train_X_summary=shap.sample(train_X, 3000)\n",
    "    #masker = shap.maskers.Independent(data = train_X_summary)\n",
    "    #explainer = shap.KernelExplainer(predict_model.predict, train_X_summary, masker = masker) \n",
    "    #explainer = shap.SamplingExplainer(predict_model.predict, train_X)\n",
    "    #explainer = shap.PermutationExplainer(predict_model.predict)\n",
    "    #explainer = shap.DeepExplainer(predict_model.predict, train_X_summary) \n",
    "    #joblib.dump(explainer, filename=file_to_save)\n",
    "    ##explainer = shap.Explainer(predict_model.predict, train_X)\n",
    "    ##shap_values2 = explainer.shap_values(train_X.head(2))\n",
    "    ##shap.plots.waterfall(shap_values2[0])\n",
    "    ##explain_model=joblib.load(filename=file_to_save)\n",
    "    ##shap_values = explain_model(train_X.head(2))\n",
    "    ##shap.plots.waterfall(shap_values[0])\n",
    "\n",
    "\n",
    "\n",
    "def create_save_explain_model(file_to_save_p, explain_method = \"LIME\", predict_model_file=None, is_train_x_dense_p=False):\n",
    "    if explain_method == \"LIME\":\n",
    "        create_save_explain_model_LIME(file_to_save=file_to_save_p, is_train_x_dense=is_train_x_dense_p);\n",
    "    elif explain_method == \"SHAP\":\n",
    "        create_save_explain_model_SHAP(file_to_save=file_to_save_p, predict_model_file_p=predict_model_file, is_train_x_dense=is_train_x_dense_p)\n",
    "    else:\n",
    "        print(\"Not supported explain method now !\\n\")\n",
    "        exit()\n",
    "\n",
    "#create_save_explain_model(explain_method = \"SHAP\", file_to_save_p=explain_model_save_file_shap_mlp,     predict_model_file=predict_model_save_file_mlp)\n",
    "#create_save_explain_model(explain_method = \"LIME\", file_to_save_p=explain_model_save_file_lime_mlp,     predict_model_file=predict_model_save_file_mlp)\n",
    "\n",
    "#create_save_explain_model(explain_method = \"LIME\", file_to_save_p=explain_model_save_file_lime_xgboost, predict_model_file=predict_model_save_file_xgboost)\n",
    "#create_save_explain_model(explain_method = \"SHAP\", file_to_save_p=explain_model_save_file_shap_xgboost, predict_model_file=predict_model_save_file_xgboost)\n",
    "\n",
    "        \n",
    "## This is no need to train SHAP with Tabnet, it is created in IOD-app druing running\n",
    "##create_save_explain_model(explain_method = \"SHAP\", file_to_save_p=explain_model_save_file_shap_tabnet,  predict_model_file=predict_model_save_file_tabnet, is_train_x_dense_p=True)\n",
    "## comment out above line      \n",
    "create_save_explain_model(explain_method = \"LIME\", file_to_save_p=explain_model_save_file_lime_tabnet,  predict_model_file=predict_model_save_file_tabnet)\n",
    "\n",
    "\n",
    "#create_save_explain_model(explain_method = \"SHAP\", file_to_save_p=explain_model_save_file_shap_lightgbm, predict_model_file=predict_model_save_file_lightgbm)\n",
    "#create_save_explain_model(explain_method = \"LIME\", file_to_save_p=explain_model_save_file_lime_lightgbm, predict_model_file=predict_model_save_file_lightgbm)\n",
    "\n",
    "\n",
    "#create_save_explain_model(explain_method = \"SHAP\", file_to_save_p=explain_model_save_file_shap_catboost, predict_model_file=predict_model_save_file_catboost)\n",
    "#create_save_explain_model(explain_method = \"LIME\", file_to_save_p=explain_model_save_file_lime_catboost, predict_model_file=predict_model_save_file_catboost)\n",
    "\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9ca71-e0c5-43eb-bd9b-cc0bfc546582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
