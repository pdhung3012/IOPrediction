{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92dd0774-9c9e-4be4-bc49-55f62c7d4af0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49790/930106852.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloadtxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kernel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKernel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKernelExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSampling\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSamplingExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/shap/explainers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_permutation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_partition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_gpu_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPUTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_exact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/shap/explainers/_permutation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartition_tree_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaskedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaskers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/shap/maskers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_masker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMasker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tabular\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndependent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImpute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fixed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/shap/maskers/_image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mrecord_import_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"torch could not be imported!\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_einsum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lowrank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvd_lowrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_lowrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from .overrides import (\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .parameter import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mUninitializedBuffer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/cori/3.9-anaconda-2021.11/lib/python3.9/site-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mupsampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUpsamplingNearest2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpsamplingBilinear2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPairwiseDistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCosineSimilarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnfold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0madaptive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdaptiveLogSoftmaxWithLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/cori-2022q1/sw/python/3.9-anaconda-2021.11/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###\n",
    "# https://docs.google.com/document/d/187FR1SXKftzRAX3xaxJo1KoFoTZ_Db4MmkJdrf9jdFc/edit\n",
    "###\n",
    "\n",
    "# AIIO Copyright (c) 2023, The Regents of the University of California,\n",
    "# through Lawrence Berkeley National Laboratory (subject to receipt of\n",
    "# any required approvals from the U.S. Dept. of Energy) and Ohio State\n",
    "# University. All rights reserved.\n",
    "\n",
    "import dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os.path\n",
    "import subprocess\n",
    "import joblib\n",
    "import shap\n",
    "from scipy import stats\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# /project/projectdirs/m2621/jeanbez/darshan/2019-01/1/Darshan_Total.csv\n",
    "# file_tot=\"/project/projectdirs/m2621/jeanbez/darshan/2019-01/1/Darshan_Total.csv\"\n",
    "\n",
    "tot_dop_cols = ['darshan_log_version', 'exe', 'uid', 'start_time', 'start_time_asci',\n",
    "                'end_time', 'end_time_asci', 'runtime', 'LUSTRE_OSTS', 'LUSTRE_MDTS', 'LUSTRE_STRIPE_OFFSET']\n",
    "\n",
    "tot_dop_cols_min = ['darshan_log_version', 'start_time', 'end_time',\n",
    "                    'runtime', 'LUSTRE_OSTS', 'LUSTRE_MDTS', 'LUSTRE_STRIPE_OFFSET']\n",
    "\n",
    "##top_dop_cols_fast_slow=['total_POSIX_FASTEST_RANK', 'total_POSIX_FASTEST_RANK_BYTES','total_POSIX_SLOWEST_RANK', 'total_POSIX_SLOWEST_RANK_BYTES', 'total_POSIX_F_VARIANCE_RANK_TIME', 'total_POSIX_F_VARIANCE_RANK_BYTES']\n",
    "# file_raw=\"/project/projectdirs/m2621/jeanbez/darshan/2019-01/1/Darshan.csv\"\n",
    "\n",
    "\n",
    "##\n",
    "##POSIX_PERF_MIBS, MPIIO_PERF_MIBS, STDIO_PERF_MIBS\n",
    "##\n",
    "def split_df_by_perf(df_p, print_header_p=True):\n",
    "    df_posix_all = df_p[df_p['POSIX_PERF_MIBS'] > 0]\n",
    "    # find MPI-IO first\n",
    "    df_mpiio = df_p[df_p['MPIIO_PERF_MIBS'] > 0]\n",
    "    # Find POSIX from NON-MPIIO\n",
    "    df_no_mpiio = df_p[df_p['MPIIO_PERF_MIBS'] <= 0]\n",
    "    df_posix = df_no_mpiio[df_no_mpiio['POSIX_PERF_MIBS'] > 0]\n",
    "    # Left are STDIO\n",
    "    df_stdio = df_no_mpiio[df_no_mpiio['POSIX_PERF_MIBS'] <= 0]\n",
    "    if print_header_p:\n",
    "        print(\"\\n Number of MPIIO job : \", df_mpiio.shape[0])\n",
    "        print(\"\\n Number of POSIX job : \", df_posix.shape[0])\n",
    "        print(\"\\n Number of STDIO job : \", df_stdio.shape[0])\n",
    "        print(\"\\n Number of POSIX(all) job : \", df_posix_all.shape[0])\n",
    "    return df_mpiio, df_posix, df_stdio, df_posix_all\n",
    "\n",
    "# ## POSIX_GROUP       \"['POSIX' 'LUSTRE' 'STDIO']\",   \"['POSIX' 'STDIO']\",  \"['POSIX']\",\n",
    "# ##\n",
    "# ## STDIO_Group       \"['LUSTRE' 'STDIO']\",          \"['STDIO']\",\n",
    "# ##\n",
    "# ## MPIIO-Group        \"['POSIX' 'MPI-IO' 'LUSTRE' 'STDIO']\",\n",
    "# ##                   \"['POSIX' 'MPI-IO' 'PNETCDF' 'LUSTRE' 'STDIO']\",\n",
    "# ##                   \"['POSIX' 'MPI-IO']\"]\n",
    "# def split_df(df_p, drop_module=True):\n",
    "#     #display(df_p['module'].apply(lambda x: str(x)).unique())\n",
    "#     df_mpiio=df_p[df_p['module'].str.contains('MPI-IO', regex=False) == True]\n",
    "#     df_no_mpiio=df_p[df_p['module'].str.contains('MPI-IO', regex=False) == False]\n",
    "#     df_posix=df_no_mpiio[df_no_mpiio['module'].str.contains('POSIX', regex=False) == True]\n",
    "#     df_stdio=df_no_mpiio[df_no_mpiio['module'].str.contains('POSIX', regex=False) == False]\n",
    "#     if drop_module:\n",
    "#         df_mpiio=df_mpiio.drop('module', axis=1)\n",
    "#         df_posix=df_posix.drop('module', axis=1)\n",
    "#         df_stdio=df_stdio.drop('module', axis=1)\n",
    "#     return df_mpiio, df_posix, df_stdio\n",
    "\n",
    "\n",
    "def read_total(file_tot, tot_dop_cols_p, print_header_p=False, is_split_p=False):\n",
    "    #print(\"Read : \", file_tot)\n",
    "    df_tot = pd.read_csv(file_tot)\n",
    "    #print(\"\\n Total Jobs : \", df_tot.shape[0])\n",
    "    if print_header_p:\n",
    "        my_list = list(df_tot)\n",
    "        print(\"Header after Darshan_Total: \", my_list)\n",
    "        # display(df_tot.head(2))\n",
    "    df_tot.drop(tot_dop_cols_p, axis=1, inplace=True)\n",
    "    df_tot.set_index('jobid', inplace=True)\n",
    "    df_tot = df_tot.sort_index()\n",
    "    ##df_tot.loc[df_tot < 0, df.select_dtypes(np.number).columns] = 0\n",
    "    ##num = df._get_numeric_data()\n",
    "    df_tot_num = df_tot._get_numeric_data()\n",
    "    df_tot_num[df_tot_num < 0] = 0\n",
    "    df_tot.fillna(0)\n",
    "    if print_header_p:\n",
    "        print(\"Total Jobs from \", file_tot, \" :\", df_tot.shape[0])\n",
    "    if is_split_p:\n",
    "        return split_df_by_perf(df_tot, print_header_p=print_header_p)\n",
    "    else:\n",
    "        return df_tot\n",
    "\n",
    "#\n",
    "# both file_start_index/file_end_index are inclusive (1, 3) = 1, 2 3\n",
    "\n",
    "\n",
    "def read_total_multiple_days(file_dir_p, file_start_index_p, file_end_index_p,  tot_dop_cols_p=tot_dop_cols, split_index=3, print_header_p=False, is_split_p=False):\n",
    "    print(tot_dop_cols_p)\n",
    "    df_result = pd.DataFrame()\n",
    "    for file_index in range(file_start_index_p, file_end_index_p+1):\n",
    "        df_tmp = read_total(file_dir_p+'/'+str(file_index)+'.csv', tot_dop_cols_p,\n",
    "                            print_header_p=print_header_p, is_split_p=is_split_p)[split_index]\n",
    "        # df_result=df_result.append(df_tmp)\n",
    "        df_result = pd.concat([df_result, df_tmp], sort=False)\n",
    "    print(\"read_total_multiple_days:  df_result's size  = \", df_result.shape)\n",
    "    return df_result\n",
    "\n",
    "#\n",
    "# both file_start_index/file_end_index are inclusive (1, 3) = 1, 2 3\n",
    "\n",
    "\n",
    "def read_total_multiple_monthes(file_dir_p, file_start_index_p, file_end_index_p,  tot_dop_cols_p=tot_dop_cols, split_index=3, print_header_p=False, is_split_p=False, print_progress=False):\n",
    "    print(tot_dop_cols_p)\n",
    "    df_result = pd.DataFrame()\n",
    "    for month_index in range(file_start_index_p, file_end_index_p+1):\n",
    "        if print_progress:\n",
    "            print(file_dir_p+'/'+str(month_index)+'/...')\n",
    "        for day_index in range(1, 32):\n",
    "            # print(file_dir_p+'/'+str(month_index)+'/'+str(day_index)+'.csv')\n",
    "            if os.stat(file_dir_p+'/'+str(month_index)+'/'+str(day_index)+'.csv').st_size == 0:\n",
    "                continue\n",
    "            df_tmp = read_total(file_dir_p+'/'+str(month_index)+'/'+str(day_index)+'.csv',\n",
    "                                tot_dop_cols_p, print_header_p=print_header_p, is_split_p=is_split_p)[split_index]\n",
    "            df_result = pd.concat([df_result, df_tmp], sort=False)\n",
    "    print(\"read_total_multiple_days:  df_result's size  = \", df_result.shape)\n",
    "    return df_result\n",
    "\n",
    "#POSIX_PERF_MIBS, MPIIO_PERF_MIBS, STDIO_PERF_MIBS\n",
    "\n",
    "\n",
    "def sort_performance_descending(df_p, is_normalize_by_proc_osts=False, add_tag=False, n_tag_groups=3, drop_perf_cols=False, only_posix=True, drop_low_performance_job_ratio=0.2, plot_hist_perf=False, use_performance_as_group=False, even_group=False, keep_performance_p=False, is_convert_to_gbs=False):\n",
    "    df_pp = df_p  # Make a copy to avoid change previous ones\n",
    "    #df[\"C\"] = df[[\"A\", \"B\"]].max(axis=1)\n",
    "    if only_posix:\n",
    "        df_pp['performance'] = df_pp['POSIX_PERF_MIBS']\n",
    "    else:\n",
    "        df_pp['performance'] = df_pp[['POSIX_PERF_MIBS',\n",
    "                                      'MPIIO_PERF_MIBS', 'STDIO_PERF_MIBS']].max(axis=1)\n",
    "    if is_normalize_by_proc_osts:\n",
    "        df_pp['performance'] = df_pp['performance'] / df_pp['nprocs']\n",
    "        #df_pp['performance'] = df_pp['performance'] / df_pp['LUSTRE_STRIPE_WIDTH']\n",
    "\n",
    "    if is_convert_to_gbs:\n",
    "        df_pp['performance'] = df_pp['performance'] / 1024\n",
    "\n",
    "    df_pp.sort_values('performance', inplace=True, ascending=False)\n",
    "    # display(df_pp.head(10))\n",
    "    # display(df_pp.tail(10))\n",
    "    if drop_low_performance_job_ratio > 0:\n",
    "        n_to_dop = int(df_pp.shape[0] * drop_low_performance_job_ratio)\n",
    "        df_pp.drop(df_pp.tail(n_to_dop).index, inplace=True)\n",
    "    # display(df_pp.head(10))\n",
    "    # display(df_pp.tail(10))\n",
    "    if plot_hist_perf:\n",
    "        df_pp.hist(column='performance')\n",
    "        #ax.set_xlabel(\"MB/sec (plot by sort_performance_descending)\")\n",
    "        # ax.set_ylabel(\"Count\")\n",
    "    if add_tag:\n",
    "        if use_performance_as_group:\n",
    "            df_pp['group'] = df_pp['performance']\n",
    "        else:\n",
    "            if even_group == False:\n",
    "                max_perf = df_pp.max(axis=0)['performance']\n",
    "                min_perf = df_pp.min(axis=0)['performance']\n",
    "                print(\"max_perf=\", max_perf)\n",
    "                print(\"min_perf=\", min_perf)\n",
    "                group_length = (max_perf - min_perf)/n_tag_groups\n",
    "                group_bound = []\n",
    "                for i in range(1, n_tag_groups):\n",
    "                    group_bound.append(min_perf + i * group_length)\n",
    "                group_bound.append(max_perf)\n",
    "                print(group_bound)\n",
    "                # create a list of our conditions\n",
    "                conditions = [\n",
    "                    (df_pp['performance'] <= group_bound[0]),\n",
    "                    (df_pp['performance'] > group_bound[0]) & (\n",
    "                        df_pp['performance'] <= group_bound[1]),\n",
    "                    (df_pp['performance'] > group_bound[1]) & (\n",
    "                        df_pp['performance'] <= group_bound[2]),\n",
    "                ]\n",
    "                # create a list of the values we want to assign for each condition\n",
    "                values = [0, 1, 2]\n",
    "                # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "                df_pp['group'] = np.select(conditions, values)\n",
    "            else:\n",
    "                g = np.linspace(0, n_tag_groups,\n",
    "                                df_pp.shape[0], endpoint=False).astype(int)\n",
    "                df_pp['group'] = g\n",
    "    if drop_perf_cols:\n",
    "        if only_posix:\n",
    "            df_pp.drop(['POSIX_PERF_MIBS'], axis=1, inplace=True)\n",
    "        else:\n",
    "            df_pp.drop(['POSIX_PERF_MIBS', 'MPIIO_PERF_MIBS',\n",
    "                       'STDIO_PERF_MIBS'], axis=1, inplace=True)\n",
    "    # display(df_pp.head(3))\n",
    "    if keep_performance_p == False:\n",
    "        df_pp.drop('performance', axis=1, inplace=True)\n",
    "    return df_pp\n",
    "\n",
    "\n",
    "def drop_cols(df_p, reg_pattern):\n",
    "    df_dropped = df_p[df_p.columns.drop(list(df_p.filter(regex=reg_pattern)))]\n",
    "    print(\"Have [\", df_dropped.shape[1], \"] colums after droping\",\n",
    "          reg_pattern, \" from orginal [\", df_p.shape[1],  \"] cols\")\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def keep_cols(df_p, filter_str_to_keep):\n",
    "    # keep valueLUSTRE_OSTS, nprocs\n",
    "    df_filtered = df_p.filter(like=filter_str_to_keep, axis=1).copy()\n",
    "    # df_filtered['valueLUSTRE_OSTS', 'nprocs']]=df_p[['valueLUSTRE_OSTS', 'nprocs']],\n",
    "    df_filtered['valueLUSTRE_OSTS'] = df_p['valueLUSTRE_OSTS'].values\n",
    "    df_filtered['nprocs'] = df_p['nprocs'].values\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def normalize_cols(df_p, method_p=\"StandardScaler\", cols=None, scaler_p=None):\n",
    "    if method_p == \"StandardScaler\":\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "    # display(df.head(3))\n",
    "    if cols is None:\n",
    "        df_p_columns = df_p.columns\n",
    "    else:\n",
    "        df_p_columns = cols\n",
    "    df_p = df_p.astype(float)\n",
    "    # print(df_p.dtypes)\n",
    "    ##df_tmp = pd.DataFrame(scaler.fit_transform(df_p.values), index=df_p.index, columns=df_p.columns)\n",
    "    df_tmp = pd.DataFrame(scaler.fit_transform(\n",
    "        df_p[df_p_columns].to_numpy()), index=df_p.index, columns=df_p_columns)\n",
    "    # print(df_tmp.dtypes)\n",
    "    df_tmp.columns = df_p_columns\n",
    "    scaler_p = scaler\n",
    "    return df_tmp\n",
    "\n",
    "\n",
    "def filter_data(df_p, drop_by_size_ratio=0.2, drop_non_luster_access=True, drop_single_core=True, drop_time_related_counter=True, drop_default_striping=False, is_drop_zero_col=True):\n",
    "    ##\n",
    "    # Drop single-core jobs\n",
    "    ##\n",
    "    if drop_single_core:\n",
    "        original_n = df_p.shape[0]\n",
    "        print(\"original_n =\", original_n)\n",
    "        df_p = df_p.loc[(df_p['nprocs'] > 1)].copy()\n",
    "        print(\"Drop [\", original_n - df_p.shape[0], \"]  among  \",\n",
    "              original_n, \" Rows by drop_single_core\")\n",
    "\n",
    "    df_p = df_p.drop(['total_POSIX_MODE'], axis=1, errors='ignore').copy()\n",
    "    print('Drop [total_POSIX_MODE] ')\n",
    "    ##\n",
    "    # Drop zero cols\n",
    "    ##\n",
    "    if is_drop_zero_col:\n",
    "        orig_cols_list = list(df_p)\n",
    "        df_p = df_p.loc[:, (df_p != 0).any(axis=0)].copy()\n",
    "        new_cols_list = list(df_p)\n",
    "        print(\"# of Cols after drop zeros = (\", len(new_cols_list), \"/ # of in orig = \",\n",
    "              len(orig_cols_list),  \"): \", list(set(orig_cols_list) - set(new_cols_list)))\n",
    "    ##\n",
    "    # filter the size  (drop 20% of the bottom )\n",
    "    ## (total_POSIX_BYTES_WRITTEN + total_POSIX_BYTES_READ)\n",
    "    ##\n",
    "    if drop_by_size_ratio > 0:\n",
    "        df_p['total_POSIX_BYTES'] = df_p['total_POSIX_BYTES_WRITTEN'] + \\\n",
    "            df_p['total_POSIX_BYTES_READ']\n",
    "        df_p = df_p.sort_values('total_POSIX_BYTES', ascending=False)\n",
    "        n_to_dop = int(df_p.shape[0] * drop_by_size_ratio)\n",
    "        print(\"Drop [\", drop_by_size_ratio * 100, \"%] = \", n_to_dop, \"/\",\n",
    "              df_p.shape[0], \" rows by total_POSIX_BYTES_WRITTEN+ total_POSIX_BYTES_READ\")\n",
    "        # print(df_p.shape)\n",
    "        # df_p=df_p.drop(df_p.tail(n_to_dop).index)\n",
    "        df_p = df_p.iloc[:-n_to_dop]\n",
    "        df_p.drop('total_POSIX_BYTES', axis=1, inplace=True)\n",
    "        # print(df_p.shape)\n",
    "    ##\n",
    "    # filter by data place Cori Home Directory, only consider accessing data from Luster file system\n",
    "    # LUSTRE_STRIPE_SIZE  LUSTRE_STRIPE_WIDTH\n",
    "    ##\n",
    "    if drop_non_luster_access:\n",
    "        original_n = df_p.shape[0]\n",
    "        df_p = df_p.loc[(df_p['LUSTRE_STRIPE_SIZE'] > 0) &\n",
    "                        (df_p['LUSTRE_STRIPE_WIDTH'] > 0)].copy()\n",
    "        print(\"Drop [\", original_n - df_p.shape[0], \"]  among  \",\n",
    "              original_n, \" Rows by drop_non_luster_access\")\n",
    "    ##\n",
    "    # Drop time-related counter\n",
    "    # total_POSIX_MAX_READ_TIME_SIZE   total_POSIX_MAX_WRITE_TIME_SIZE  total_POSIX_F_OPEN_START_TIMESTAMP\n",
    "    # total_POSIX_F_READ_START_TIMESTAMP total_POSIX_F_WRITE_START_TIMESTAMP total_POSIX_F_CLOSE_START_TIMESTAMP\n",
    "    # total_POSIX_F_OPEN_END_TIMESTAMP total_POSIX_F_READ_END_TIMESTAMP total_POSIX_F_WRITE_END_TIMESTAMP\n",
    "    # total_POSIX_F_CLOSE_END_TIMESTAMP total_POSIX_F_READ_TIME total_POSIX_F_WRITE_TIME total_POSIX_F_META_TIME\n",
    "    # total_POSIX_F_MAX_READ_TIME total_POSIX_F_MAX_WRITE_TIME total_POSIX_F_FASTEST_RANK_TIME total_POSIX_F_SLOWEST_RANK_TIME\n",
    "    if drop_time_related_counter:\n",
    "        orig_cols_list = list(df_p)\n",
    "        cols_without_TIMESTAMP = [\n",
    "            x for x in list(df_p) if 'TIMESTAMP' not in x]\n",
    "        df_p = df_p[cols_without_TIMESTAMP]\n",
    "        other_dop_time_cols = ['total_POSIX_F_READ_TIME', 'total_POSIX_F_WRITE_TIME', 'total_POSIX_F_META_TIME', 'total_POSIX_F_MAX_READ_TIME', 'total_POSIX_F_MAX_WRITE_TIME', 'total_POSIX_F_FASTEST_RANK_TIME',  'total_POSIX_F_SLOWEST_RANK_TIME', 'total_POSIX_FASTEST_RANK',\n",
    "                               'total_POSIX_FASTEST_RANK_BYTES', 'total_POSIX_SLOWEST_RANK', 'total_POSIX_SLOWEST_RANK_BYTES', 'total_POSIX_F_VARIANCE_RANK_TIME', 'total_POSIX_MAX_BYTE_READ', 'total_POSIX_MAX_BYTE_WRITTEN', 'total_POSIX_MAX_READ_TIME_SIZE', 'total_POSIX_MAX_WRITE_TIME_SIZE']\n",
    "        df_p = df_p.drop(other_dop_time_cols, axis=1, errors='ignore').copy()\n",
    "        new_cols_list = list(df_p)\n",
    "        print(\"# of Cols after drop TIME (\", len(new_cols_list), \"/\",\n",
    "              len(orig_cols_list),  \"): \", list(set(orig_cols_list) - set(new_cols_list)))\n",
    "    ##\n",
    "    # Drop default striping setting\n",
    "    ##\n",
    "    # if drop_default_striping:\n",
    "    ##    original_n = df_p.shape[0]\n",
    "    ##    df_p=df_p.loc[(df_p['LUSTRE_STRIPE_SIZE'] > 1) & (df_p['LUSTRE_STRIPE_WIDTH'] > 0)].copy()\n",
    "    ##    print(\"Drop [\", original_n - df_p.shape[0],\"]  among  \" , original_n, \" Rows by drop_non_luster_access\")\n",
    "\n",
    "    print(\"After filter,  shape is : \", df_p.shape)\n",
    "    return df_p\n",
    "\n",
    "\n",
    "#POSIX_PERF_MIBS, MPIIO_PERF_MIBS, STDIO_PERF_MIBS\n",
    "def add_tag(df_p, only_posix=False,  is_normalize_by_proc_osts=False,  is_convert_to_gbs=False, drop_low_performance_job_ratio=0, plot_hist_perf=False,  drop_orig_perf_cols=False, is_group_tag=False, n_groups=3, even_group=False, keep_performance_p=False, n_to_display=2):\n",
    "    df_pp = df_p\n",
    "    if only_posix:\n",
    "        df_pp['performance'] = df_pp['POSIX_PERF_MIBS']\n",
    "    else:\n",
    "        df_pp['performance'] = df_pp[['POSIX_PERF_MIBS',\n",
    "                                      'MPIIO_PERF_MIBS', 'STDIO_PERF_MIBS']].max(axis=1)\n",
    "    if is_normalize_by_proc_osts:\n",
    "        df_pp['performance'] = df_pp['performance'] / df_pp['nprocs']\n",
    "    if is_convert_to_gbs:\n",
    "        df_pp['performance'] = df_pp['performance'] / 1024\n",
    "    if drop_low_performance_job_ratio > 0:\n",
    "        df_pp.sort_values('performance', inplace=True, ascending=False)\n",
    "        n_to_dop = int(df_pp.shape[0] * drop_low_performance_job_ratio)\n",
    "        df_pp.drop(df_pp.tail(n_to_dop).index, inplace=True)\n",
    "    if plot_hist_perf:\n",
    "        df_pp.hist(column='performance')\n",
    "    if is_group_tag == False:\n",
    "        df_pp['tag'] = df_pp['performance']\n",
    "    else:\n",
    "        if even_group == False:\n",
    "            max_perf = df_pp.max(axis=0)['performance']\n",
    "            min_perf = df_pp.min(axis=0)['performance']\n",
    "            print(\"max_perf=\", max_perf)\n",
    "            print(\"min_perf=\", min_perf)\n",
    "            group_length = (max_perf - min_perf)/n_groups\n",
    "            group_bound = []\n",
    "            for i in range(1, n_groups):\n",
    "                group_bound.append(min_perf + i * group_length)\n",
    "            group_bound.append(max_perf)\n",
    "            print(group_bound)\n",
    "            # create a list of our conditions\n",
    "            conditions = [\n",
    "                (df_pp['performance'] <= group_bound[0]),\n",
    "                (df_pp['performance'] > group_bound[0]) & (\n",
    "                    df_pp['performance'] <= group_bound[1]),\n",
    "                (df_pp['performance'] > group_bound[1]) & (\n",
    "                    df_pp['performance'] <= group_bound[2]),\n",
    "            ]\n",
    "            # create a list of the values we want to assign for each condition\n",
    "            values = [0, 1, 2]\n",
    "            # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            df_pp['tag'] = np.select(conditions, values)\n",
    "        else:\n",
    "            g = np.linspace(\n",
    "                0, n_groups, df_pp.shape[0], endpoint=False).astype(int)\n",
    "            df_pp['tag'] = g\n",
    "    if drop_orig_perf_cols:\n",
    "        if only_posix:\n",
    "            df_pp.drop(['POSIX_PERF_MIBS'], axis=1, inplace=True)\n",
    "        else:\n",
    "            df_pp.drop(['POSIX_PERF_MIBS', 'MPIIO_PERF_MIBS',\n",
    "                       'STDIO_PERF_MIBS'], axis=1, inplace=True)\n",
    "    # display(df_pp.head(n_to_display))\n",
    "    if keep_performance_p == False:\n",
    "        df_pp.drop('performance', axis=1, inplace=True)\n",
    "    return df_pp\n",
    "\n",
    "\n",
    "def transform_raw_data(df_p, file_to_save=None, n_to_display_p=2):\n",
    "    df_tot_p = drop_cols(df_p, \"MPIIO\")\n",
    "    df_tot_p = drop_cols(df_tot_p, \"STDIO\")\n",
    "    df_tot_p = filter_data(df_tot_p, drop_by_size_ratio=0,\n",
    "                           drop_single_core=False, is_drop_zero_col=False)\n",
    "    # df_tot_p_orig=df_tot_p\n",
    "    df_tot_p = add_tag(df_tot_p, only_posix=True,\n",
    "                       drop_orig_perf_cols=True, n_to_display=n_to_display_p)\n",
    "    # if log_transform:\n",
    "    #    df_tot_p=np.log10(df_tot_p + 0.00001) ##Log transform\n",
    "    # if log_transform_tag:\n",
    "    #    df_tot_p[\"tag\"]=np.log10(df_tot_p[\"tag\"])\n",
    "    # df_tot_p=df_tot_p.sample(frac=1)  ## Random shuffter the data\n",
    "    if file_to_save is not None:\n",
    "        df_tot_p.to_csv(file_to_save, header=True, index=False)\n",
    "    return df_tot_p\n",
    "\n",
    "\n",
    "def parser_darshan(darshan_file_name):\n",
    "    output_csv_file = darshan_file_name+'.csv'\n",
    "    temp_directory = darshan_file_name+'-tdir'\n",
    "    subprocess.run([\"./parser.sh\", darshan_file_name,\n",
    "                   output_csv_file, temp_directory])\n",
    "    #subprocess.run([\"rm\", \"-rf\", temp_directory])\n",
    "    return output_csv_file\n",
    "\n",
    "\n",
    "def load_explain_model(explain_model_save_file_p, method=\"LIME\"):\n",
    "    if method == \"LIME\":\n",
    "        with open(explain_model_save_file_p, 'rb') as f:\n",
    "            explain_model = dill.load(f)\n",
    "        return explain_model\n",
    "    elif method == \"SHAP\":\n",
    "        explain_model = joblib.load(filename=explain_model_save_file_p)\n",
    "        return explain_model\n",
    "    else:\n",
    "        print(\"Not supported exlain method now !\\n\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "def load_predict_model(predict_model_save_file_p, isTabnet=False):\n",
    "    if isTabnet == False:\n",
    "        predict_model = joblib.load(predict_model_save_file_p)\n",
    "        return predict_model\n",
    "    else:\n",
    "        predict_model = TabNetRegressor(seed=42)\n",
    "        predict_model.load_model(predict_model_save_file_p)\n",
    "        return predict_model\n",
    "\n",
    "\n",
    "def explain_df_with_LIME(explain_model, predict_model, explain_X, explain_Y, header,  run_web=False):\n",
    "    # Build the LimeTabularExplainer\n",
    "    explanation = explain_model.explain_instance(\n",
    "        explain_X, predict_model.predict, num_features=len(explain_X))\n",
    "    print(\"Prediction : \",   predict_model.predict(explain_X.reshape(1, -1)))\n",
    "    print(\"Empirically estimated :     \", explain_Y)\n",
    "    if run_web == False:\n",
    "        explanation.show_in_notebook()\n",
    "        return explanation.as_list()\n",
    "        # display(explanation.as_list())\n",
    "    else:\n",
    "        # explanation.save_to_file('lime.html')\n",
    "        return explanation.as_list()\n",
    "\n",
    "\n",
    "def explain_df_with_SHAP(explain_model, predict_model, explain_X, explain_Y, header, run_web=False):\n",
    "    df = pd.DataFrame([explain_X], columns=header)\n",
    "    # display(df)\n",
    "    shap_values = explain_model(df)\n",
    "    # print(shap_values[0])\n",
    "    if run_web == False:\n",
    "        shap.plots.waterfall(shap_values[0])\n",
    "        return shap_values[0]\n",
    "        # shap.initjs()\n",
    "        ##shap.plots.force(shap_values[0], matplotlib=True)\n",
    "    else:\n",
    "        # shap.initjs()\n",
    "        ##output_of_force_plot=shap.plots.force(shap_values[0], show=False)\n",
    "        # shap.plots.force(shap_values[0])\n",
    "        ##shap.save_html(\"shap.html\", output_of_force_plot)\n",
    "        return shap_values[0]\n",
    "    # https://stackoverflow.com/questions/65837159/how-to-get-the-shap-values-of-each-feature\n",
    "    #f=shap.force_plot(explainer.expected_value, shap_values, X, show=False)\n",
    "    #shap.save_html(\"index.htm\", f)\n",
    "    # shap.plots.force(shap_values[0])\n",
    "    #\n",
    "    #shap_values = model.shap_values(explain_X)\n",
    "    # model.initjs()\n",
    "    #model.force_plot(model.expected_value, shap_values, explain_X, feature_names=features)\n",
    "\n",
    "\n",
    "def explain_io(explain_model_p,  predict_model_p, explain_X_p, explain_Y_p, header_p, explain_method=\"LIME\", run_web_p=False):\n",
    "    if explain_method == \"LIME\":\n",
    "        return explain_df_with_LIME(explain_model=explain_model_p, predict_model=predict_model_p, header=header_p, explain_X=explain_X_p, explain_Y=explain_Y_p, run_web=run_web_p)\n",
    "    elif explain_method == \"SHAP\":\n",
    "        return explain_df_with_SHAP(explain_model=explain_model_p, predict_model=predict_model_p, header=header_p, explain_X=explain_X_p, explain_Y=explain_Y_p, run_web=run_web_p)\n",
    "    else:\n",
    "        print(\"Not supported exlain method now !\\n\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "# Process darshan log\n",
    "def get_counters_from_a_darshan_file(darshan_file_to_explain_p, train_feature_name_list=None):\n",
    "    darshan_file_to_explain_csv = parser_darshan(darshan_file_to_explain_p)\n",
    "    darshan_file_to_explain_df = read_total(\n",
    "        darshan_file_to_explain_csv, print_header_p=False, is_split_p=False, tot_dop_cols_p=tot_dop_cols)\n",
    "    #darshan_file_to_explain_df_transformed, df_orig=transform_raw_data(df_p=darshan_file_to_explain_df, n_to_display_p =3)\n",
    "    darshan_file_to_explain_df_transformed = transform_raw_data(\n",
    "        df_p=darshan_file_to_explain_df, n_to_display_p=3)\n",
    "    df_orig = darshan_file_to_explain_df_transformed\n",
    "    darshan_file_to_explain_df_transformed = feature_engineering(\n",
    "        darshan_file_to_explain_df_transformed)\n",
    "    if train_feature_name_list is not None:\n",
    "        darshan_file_to_explain_df_transformed = darshan_file_to_explain_df_transformed[\n",
    "            train_feature_name_list]\n",
    "    print(darshan_file_to_explain_df_transformed.shape)\n",
    "    headers = list(darshan_file_to_explain_df_transformed.columns)\n",
    "    print(headers)\n",
    "    headers.pop()\n",
    "    print(headers)\n",
    "    darshan_file_to_explain_df_transformed_np = darshan_file_to_explain_df_transformed.to_numpy()\n",
    "    n_dims = darshan_file_to_explain_df_transformed_np.shape[1]\n",
    "    explain_X = darshan_file_to_explain_df_transformed_np[:, 0:n_dims-1]\n",
    "    explain_Y = darshan_file_to_explain_df_transformed_np[:, n_dims-1]\n",
    "    print(type(explain_X))\n",
    "    return explain_X[0], explain_Y[0], headers, df_orig\n",
    "\n",
    "\n",
    "def min_log_transfer(df_p, col):\n",
    "    zeros_sum = (df_p[col] == 0).sum()\n",
    "    if zeros_sum != 0:\n",
    "        minValue = df_p.loc[df_p[col] > 0.1, col].min()\n",
    "        if minValue > 1:\n",
    "            minValue = 1\n",
    "        else:\n",
    "            minValue = minValue / 2\n",
    "        temp_col = df_p[col]\n",
    "        new_col = temp_col.replace(to_replace=0, value=minValue)\n",
    "        return np.log10(new_col)\n",
    "    else:\n",
    "        return np.log10(df_p[col])\n",
    "\n",
    "\n",
    "def describe_col(df_p, col, try_log=False, try_pluy_one_log=False):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    display(df_p[col].describe())\n",
    "    df_p[col].hist(ax=axs[0])\n",
    "    df_p.plot.scatter(x=col, y='tag', c='DarkBlue', ax=axs[1])\n",
    "    plt.plot()\n",
    "    if try_pluy_one_log:\n",
    "        zero_sum1 = (df_p[col] == 0).sum()\n",
    "        print(\"# of zeros in df = \", zero_sum1,\n",
    "              \" (\", zero_sum1/df_p.shape[0], \")\")\n",
    "        minValue = df_p.loc[df_p[col] > 0.1, col].min()\n",
    "        print(\"minValue = \", minValue)\n",
    "        df_p[col+'_log'] = plus_one_log_transfer(df_p, col)\n",
    "        display(df_p[col+'_log'].describe())\n",
    "        df_p[col+'_log'].hist(ax=axs[2])\n",
    "\n",
    "\n",
    "def plus_one_log_transfer(df_p, col, log_flag=True):\n",
    "    if log_flag:\n",
    "        temp_col = df_p[col]\n",
    "        return np.log10(temp_col+1)\n",
    "    else:\n",
    "        return df_p[col]\n",
    "\n",
    "\n",
    "# To further delete:\n",
    "## total_POSIX_MODE, total_POSIX_MAX_BYTE_READ, total_POSIX_MAX_BYTE_WRITTEN, total_POSIX_MAX_READ_TIME_SIZE, total_POSIX_MAX_WRITE_TIME_SIZE,\n",
    "# Reference:  https://discuss.analyticsvidhya.com/t/methods-to-deal-with-zero-values-while-performing-log-transformation-of-variable/2431/9\n",
    "def feature_engineering(df_p, file_to_save=None, log_transform_feature=True, log_transform_tag=True):\n",
    "    df_result = pd.DataFrame()\n",
    "\n",
    "    # Non-zero, which can be log10 directly\n",
    "    df_result['nprocs'] = plus_one_log_transfer(\n",
    "        df_p, 'nprocs', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_OPENS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_OPENS', log_flag=log_transform_feature)\n",
    "    df_result['LUSTRE_STRIPE_SIZE'] = plus_one_log_transfer(\n",
    "        df_p, 'LUSTRE_STRIPE_SIZE', log_flag=log_transform_feature)\n",
    "    df_result['LUSTRE_STRIPE_WIDTH'] = plus_one_log_transfer(\n",
    "        df_p, 'LUSTRE_STRIPE_WIDTH', log_flag=log_transform_feature)\n",
    "\n",
    "    # Zero value, copy directly\n",
    "    #df_result['POSIX_FILENOS'] = df_p['total_POSIX_FILENOS']\n",
    "    df_result['POSIX_FILENOS'] =   plus_one_log_transfer(df_p, 'total_POSIX_FILENOS', log_flag=log_transform_feature) \n",
    "    \n",
    "    df_result['POSIX_DUPS'] = df_p['total_POSIX_DUPS']\n",
    "    df_result['POSIX_MMAPS'] = df_p['total_POSIX_MMAPS']\n",
    "    df_result['POSIX_RENAME_SOURCES'] = df_p['total_POSIX_RENAME_SOURCES']\n",
    "    df_result['POSIX_RENAME_TARGETS'] = df_p['total_POSIX_RENAME_TARGETS']\n",
    "    df_result['POSIX_RENAMED_FROM'] = df_p['total_POSIX_RENAMED_FROM']\n",
    "    df_result['POSIX_MEM_ALIGNMENT'] = plus_one_log_transfer(df_p, 'total_POSIX_MEM_ALIGNMENT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_FILE_ALIGNMENT'] = plus_one_log_transfer(df_p, 'total_POSIX_FILE_ALIGNMENT', log_flag=log_transform_feature)\n",
    "\n",
    "    # Non-zero, which can not be log10 directly\n",
    "    df_result['POSIX_READS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_READS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_WRITES'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_WRITES', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SEEKS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SEEKS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STATS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STATS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_BYTES_READ'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_BYTES_READ', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_BYTES_WRITTEN'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_BYTES_WRITTEN', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_CONSEC_READS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_CONSEC_READS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_CONSEC_WRITES'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_CONSEC_WRITES', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SEQ_READS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SEQ_READS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SEQ_WRITES'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SEQ_WRITES', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_FSYNCS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_FSYNCS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_FDSYNCS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_FDSYNCS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_RW_SWITCHES'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_RW_SWITCHES', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_MEM_NOT_ALIGNED'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_MEM_NOT_ALIGNED', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_FILE_NOT_ALIGNED'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_FILE_NOT_ALIGNED', log_flag=log_transform_feature)\n",
    "\n",
    "    df_result['POSIX_SIZE_READ_0_100'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_0_100', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_100_1K'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_100_1K', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_1K_10K'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_1K_10K', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_10K_100K'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_10K_100K', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_100K_1M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_100K_1M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_1M_4M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_1M_4M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_4M_10M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_4M_10M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_10M_100M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_10M_100M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_100M_1G'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_100M_1G', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_READ_1G_PLUS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_READ_1G_PLUS', log_flag=log_transform_feature)\n",
    "\n",
    "    df_result['POSIX_SIZE_WRITE_0_100'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_0_100', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_100_1K'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_100_1K', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_1K_10K'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_1K_10K', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_10K_100K'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_10K_100K', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_100K_1M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_100K_1M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_1M_4M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_1M_4M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_4M_10M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_4M_10M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_10M_100M'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_10M_100M', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_100M_1G'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_100M_1G', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_SIZE_WRITE_1G_PLUS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_SIZE_WRITE_1G_PLUS', log_flag=log_transform_feature)\n",
    "\n",
    "    df_result['POSIX_STRIDE1_STRIDE'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE1_STRIDE', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE2_STRIDE'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE2_STRIDE', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE3_STRIDE'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE3_STRIDE', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE4_STRIDE'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE4_STRIDE', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE1_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE1_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE2_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE2_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE3_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE3_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_STRIDE4_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_STRIDE4_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS1_ACCESS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS1_ACCESS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS2_ACCESS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS2_ACCESS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS3_ACCESS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS3_ACCESS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS4_ACCESS'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS4_ACCESS', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS1_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS1_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS2_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS2_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS3_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS3_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_ACCESS4_COUNT'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_ACCESS4_COUNT', log_flag=log_transform_feature)\n",
    "    df_result['POSIX_F_VARIANCE_RANK_BYTES'] = plus_one_log_transfer(\n",
    "        df_p, 'total_POSIX_F_VARIANCE_RANK_BYTES', log_flag=log_transform_feature)\n",
    "\n",
    "    df_result['tag'] = plus_one_log_transfer(\n",
    "        df_p, 'tag', log_flag=log_transform_tag)\n",
    "\n",
    "    #describe_col(df_p, 'total_POSIX_F_VARIANCE_RANK_BYTES', try_pluy_one_log=True)\n",
    "    if file_to_save is not None:\n",
    "        df_result.to_csv(file_to_save, header=True, index=False)\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def drop_sparse_rc(df_p, ratio_of_zero=0.75):\n",
    "    # Remove sparse rowswith more than 0.75 zeros\n",
    "    # https://stackoverflow.com/questions/37992585/how-to-remove-rows-from-a-dataframe-if-75-of-its-column-values-is-equal-to-0\n",
    "    df_temp = df_p[(~df_p.astype('bool')).mean(axis=1) < ratio_of_zero]\n",
    "    # Remove sparse column with more than 0.75 zeros\n",
    "    # https://stackoverflow.com/questions/44250642/drop-columns-with-more-than-70-zeros\n",
    "    df_temp2 = df_temp.loc[:, (df_temp == 0).mean() < ratio_of_zero]\n",
    "    print(\"After drop_sparse_rc, shape =\", df_temp2.shape)\n",
    "    return df_temp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f563d-ea99-4f7e-b0ef-d48a221ad1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
